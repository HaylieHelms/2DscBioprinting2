{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04abcd2e-d7ba-4fb5-ab0e-2e5b5964d8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import anndata as ad\n",
    "from scipy import io\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# 1) Load the files\n",
    "counts = io.mmread(\"scPrint2_PR_counts.mtx\")\n",
    "counts = counts.tocsr() if sp.issparse(counts) else sp.csr_matrix(counts)\n",
    "\n",
    "genes = pd.read_csv(\"scPrint2_PR_genes.tsv\", sep=\"\\t\", header=None)\n",
    "meta  = pd.read_csv(\"scPrint2_PR_meta.csv\", index_col=0)\n",
    "\n",
    "# Clean gene table\n",
    "genes = genes.copy()\n",
    "genes.iloc[:, 0] = genes.iloc[:, 0].astype(str).str.strip()\n",
    "\n",
    "# If there's a 1-row header artifact, drop it\n",
    "r, c = counts.shape\n",
    "g = genes.shape[0]\n",
    "\n",
    "if g == r + 1:\n",
    "    genes = genes.iloc[1:].reset_index(drop=True)\n",
    "    g = genes.shape[0]\n",
    "elif g == c + 1:\n",
    "    genes = genes.iloc[1:].reset_index(drop=True)\n",
    "    g = genes.shape[0]\n",
    "\n",
    "# Name gene column (assumes 1-col gene file; adjust if your file has 2 cols)\n",
    "if genes.shape[1] != 1:\n",
    "    raise ValueError(f\"genes.tsv has {genes.shape[1]} columns; update parsing to handle this.\")\n",
    "genes.columns = [\"gene_id\"]\n",
    "\n",
    "# Basic cleanliness for meta\n",
    "meta = meta.copy()\n",
    "meta.index = meta.index.astype(str).str.strip()\n",
    "\n",
    "print(\"counts shape:\", counts.shape)\n",
    "print(\"genes shape: \", genes.shape)\n",
    "print(\"meta shape:  \", meta.shape)\n",
    "\n",
    "# 2) Decide orientation (we want cells x genes)\n",
    "n_cells = meta.shape[0]\n",
    "n_genes = genes.shape[0]\n",
    "\n",
    "if (r == n_genes) and (c == n_cells):\n",
    "    X = counts.T.tocsr()\n",
    "    print(\"Orientation: genes x cells → transposed to cells x genes\")\n",
    "elif (r == n_cells) and (c == n_genes):\n",
    "    X = counts.tocsr()\n",
    "    print(\"Orientation: cells x genes → using as-is\")\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"Dimension mismatch:\\n\"\n",
    "        f\"  counts: {counts.shape}\\n\"\n",
    "        f\"  genes:  {n_genes}\\n\"\n",
    "        f\"  meta:   {n_cells}\\n\"\n",
    "        \"Cannot safely infer orientation. You likely need barcodes.tsv or a different export.\"\n",
    "    )\n",
    "\n",
    "# Hard dimension checks\n",
    "if X.shape[0] != n_cells:\n",
    "    raise ValueError(f\"X has {X.shape[0]} rows but meta has {n_cells} rows (cells).\")\n",
    "if X.shape[1] != n_genes:\n",
    "    raise ValueError(f\"X has {X.shape[1]} cols but genes has {n_genes} rows (genes).\")\n",
    "\n",
    "# 3) Ensure obs names are unique (required)\n",
    "preferred_id_cols = [\"barcode\", \"barcodes\", \"cell\", \"cell_id\", \"CellID\", \"CellID_unique\"]\n",
    "id_col = next((col for col in preferred_id_cols if col in meta.columns), None)\n",
    "\n",
    "if id_col is not None:\n",
    "    meta[id_col] = meta[id_col].astype(str).str.strip()\n",
    "    if meta[id_col].duplicated().any():\n",
    "        raise ValueError(\n",
    "            f\"Found '{id_col}' column but it contains duplicates. \"\n",
    "            \"Use a different unique ID column or combine sample+barcode.\"\n",
    "        )\n",
    "    meta.index = meta[id_col]\n",
    "else:\n",
    "    if meta.index.duplicated().any():\n",
    "        s = pd.Series(meta.index, index=meta.index)\n",
    "        meta.index = meta.index + \"_\" + s.groupby(level=0).cumcount().astype(str)\n",
    "        meta.index = meta.index.str.replace(r\"_0$\", \"\", regex=True)\n",
    "\n",
    "if not meta.index.is_unique:\n",
    "    raise ValueError(\"Failed to make meta index unique; inspect meta for duplicate cell IDs.\")\n",
    "\n",
    "# 4) Build AnnData with obs/var immediately\n",
    "var = pd.DataFrame(index=genes[\"gene_id\"].astype(str).values)\n",
    "adata = ad.AnnData(X=X, obs=meta, var=var)\n",
    "\n",
    "# Make names clean + unique\n",
    "adata.var_names = adata.var_names.astype(str).str.strip()\n",
    "adata.var_names_make_unique()\n",
    "\n",
    "adata.obs_names = adata.obs.index.astype(str)\n",
    "adata.obs_names_make_unique()  # should already be unique, but harmless safeguard\n",
    "\n",
    "print(adata)\n",
    "\n",
    "# Save\n",
    "adata.write_h5ad(\"scPrint2_forLIANA.h5ad\")\n",
    "print(\"Wrote file: scPrint2_forLIANA.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "160e857c-b07f-4eeb-9f7e-2745cd708b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "import liana as li\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b7f64a-992e-4b7b-bf23-bf846874e8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the h5ad we already created\n",
    "adata = ad.read_h5ad(\"scPrint2_forLIANA.h5ad\")\n",
    "\n",
    "# Make obs names unique to silence warnings\n",
    "adata.obs_names_make_unique()\n",
    "\n",
    "# Metadata standardization \n",
    "required = [\"CellType\", \"CultureCondition\", \"PrintPattern\"]\n",
    "missing = [c for c in required if c not in adata.obs.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing required obs columns: {missing}\\nFound: {adata.obs.columns.tolist()}\")\n",
    "\n",
    "def make_sample_from_printpattern(x: str) -> str:\n",
    "    x = str(x)\n",
    "    m = re.search(r\"(Pattern|Random)\\s*[_-]?\\s*(\\d+)\", x, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        return f\"{m.group(1).capitalize()}_{m.group(2)}\"\n",
    "    return x\n",
    "\n",
    "# Create per-replicate sample ID for LIANA's sample aware aggregation\n",
    "adata.obs[\"sample\"] = adata.obs[\"PrintPattern\"].map(make_sample_from_printpattern)\n",
    "\n",
    "# Create condition column\n",
    "adata.obs[\"condition\"] = adata.obs[\"CultureCondition\"].astype(str)\n",
    "adata.obs[\"condition\"] = adata.obs[\"condition\"].replace({\"pattern\": \"Pattern\", \"random\": \"Random\"})\n",
    "\n",
    "# Stable raw counts layer for rank_aggregate \n",
    "if \"counts\" not in adata.layers:\n",
    "    adata.layers[\"counts\"] = adata.X.copy()\n",
    "\n",
    "print(\"condition counts:\")\n",
    "print(adata.obs[\"condition\"].value_counts(dropna=False))\n",
    "\n",
    "print(\"\\nsample (first 20 unique):\")\n",
    "print(sorted(adata.obs[\"sample\"].unique())[:20])\n",
    "\n",
    "# Summary\n",
    "adata  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee1efc2-43fd-484b-851d-62a69449e9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only Pattern + Random cultures\n",
    "keep_cc = [\"Pattern\", \"Random\"]\n",
    "adata_cc = adata[adata.obs[\"condition\"].isin(keep_cc)].copy()\n",
    "\n",
    "# Remove any unlabeled cells\n",
    "adata_cc = adata_cc[adata_cc.obs[\"CellType\"] != \"Unlabeled\"].copy()\n",
    "\n",
    "print(\"condition counts:\")\n",
    "print(adata_cc.obs[\"condition\"].value_counts())\n",
    "\n",
    "print(\"\\nCellType counts:\")\n",
    "print(adata_cc.obs[\"CellType\"].value_counts())\n",
    "\n",
    "print(\"\\nSamples per condition:\")\n",
    "print(adata_cc.obs.groupby(\"condition\")[\"sample\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb40014-2c03-4f8c-b313-ea328f7e0ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_pat  = adata_cc[adata_cc.obs[\"condition\"] == \"Pattern\"].copy()\n",
    "adata_rand = adata_cc[adata_cc.obs[\"condition\"] == \"Random\"].copy()\n",
    "\n",
    "print(f\"Pattern cells: {adata_pat.n_obs}\")\n",
    "print(f\"Random cells:  {adata_rand.n_obs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8e8bb0f-d944-4511-bc59-d0e602937837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Help here: https://liana-py.readthedocs.io/en/latest/notebooks/basic_usage.html\n",
    "\n",
    "def run_liana_one_condition(adata_in):\n",
    "    \"\"\"\n",
    "    Run LIANA rank_aggregate on an AnnData object,\n",
    "    grouping by CellType and using the consensus LR resource.\n",
    "    \"\"\"\n",
    "    adata_work = adata_in.copy()\n",
    "\n",
    "    # Use raw counts as the starting point\n",
    "    adata_work.X = adata_work.layers[\"counts\"].copy()\n",
    "\n",
    "    sc.pp.normalize_total(adata_work, target_sum=1e4)\n",
    "    sc.pp.log1p(adata_work)\n",
    "\n",
    "    li.mt.rank_aggregate(\n",
    "        adata_work,\n",
    "        groupby=\"CellType\",\n",
    "        resource_name=\"consensus\",\n",
    "        expr_prop=0.1,\n",
    "        use_raw=False,\n",
    "        n_perms=1000,\n",
    "        inplace=True,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    return adata_work.uns[\"liana_res\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510276b0-10a8-4ce9-b289-df1c404bf83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running LIANA on Pattern...\")\n",
    "res_pat = run_liana_one_condition(adata_pat)\n",
    "\n",
    "print(\"\\nRunning LIANA on Random...\")\n",
    "res_rand = run_liana_one_condition(adata_rand)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c84dea-09d2-4097-9a70-2147c1d6d48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pattern result head:\")\n",
    "print(res_pat.head())\n",
    "\n",
    "print(\"\\nRandom result head:\")\n",
    "print(res_rand.head())\n",
    "\n",
    "res_pat.to_csv(\"scPrint2_liana_Pattern_rank_aggregate.csv\", index=False)\n",
    "res_rand.to_csv(\"scPrint2_liana_Random_rank_aggregate.csv\", index=False)\n",
    "\n",
    "print(\"\\nSaved LIANA results to CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b91fc84b-d875-4390-953a-a63d8df6dbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_top_interactions(df, n_top=30):\n",
    "    \"\"\"\n",
    "    Select the top n_top interactions from a LIANA result DataFrame.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # in LIANA, lower magnitude_rank is better\n",
    "    if \"magnitude_rank\" in df.columns:\n",
    "        df_sorted = df.sort_values(\"magnitude_rank\", ascending=True)\n",
    "    # in LIANA, lower aggregate rank is better\n",
    "    elif \"aggregate_rank\" in df.columns:\n",
    "        df_sorted = df.sort_values(\"aggregate_rank\", ascending=True)\n",
    "    # in LIANA, higher lrscore means stronger interaction\n",
    "    elif \"lrscore\" in df.columns:\n",
    "        df_sorted = df.sort_values(\"lrscore\", ascending=False)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "        \"No recognized ranking columns found. \"\n",
    "        \"Expected one of: magnitude_rank, aggregate_rank, lrscore.\"\n",
    "        )\n",
    "\n",
    "    return df_sorted.head(n_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3410d2ff-ac68-471c-b1ce-a6993634bd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert LIANA’s ligand–receptor inference results into a weighted cell–cell interaction network by aggregating interaction-level evidence between sender and receiver cell types\n",
    "\n",
    "def make_cell_cell_edges(df, weight_mode=\"count\"):\n",
    "    \"\"\"\n",
    "    Collapse LIANA LR table into a cell→cell weighted edge list.\n",
    "\n",
    "    weight_mode:\n",
    "      - \"count\"  : number of LR pairs between source/target \n",
    "      - \"sum_inv_rank\": sum(1 / magnitude_rank) per source/target\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    if weight_mode == \"count\":\n",
    "        edges = (\n",
    "            df.groupby([\"source\", \"target\"])\n",
    "              .size()\n",
    "              .reset_index(name=\"weight\")\n",
    "        )\n",
    "\n",
    "    elif weight_mode == \"sum_inv_rank\":\n",
    "        if \"magnitude_rank\" not in df.columns:\n",
    "            raise ValueError(\"magnitude_rank not in LIANA result.\")\n",
    "        df[\"inv_rank\"] = 1.0 / df[\"magnitude_rank\"].astype(float)\n",
    "        edges = (\n",
    "            df.groupby([\"source\", \"target\"])[\"inv_rank\"]\n",
    "              .sum()\n",
    "              .reset_index(name=\"weight\")\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Unknown weight_mode\")\n",
    "\n",
    "    return edges\n",
    "\n",
    "\n",
    "# Pattern and Random cell–cell edge tables\n",
    "edges_pat  = make_cell_cell_edges(res_pat,  weight_mode=\"count\")\n",
    "edges_rand = make_cell_cell_edges(res_rand, weight_mode=\"count\")\n",
    "\n",
    "print(\"Pattern edges:\")\n",
    "print(edges_pat.head())\n",
    "\n",
    "print(\"\\nRandom edges:\")\n",
    "print(edges_rand.head())\n",
    "\n",
    "# Save for R / circos\n",
    "edges_pat.to_csv(\"scPrint2_Pattern_cellcell_edges.csv\", index=False)\n",
    "edges_rand.to_csv(\"scPrint2_Random_cellcell_edges.csv\", index=False)\n",
    "\n",
    "print(\"Wrote scPrint2_Pattern_cellcell_edges.csv and scPrint2_Random_cellcell_edges.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94982e24-481f-403a-ae4d-7e19bd790d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOVED THE DATA VIZ TO R \n",
    "\n",
    "#\n",
    "\n",
    "#\n",
    "\n",
    "#\n",
    "\n",
    "# Made a Circos Plot\n",
    "# using scPrint2_Pattern_cellcell_edges.csv and scPrint2_Random_cellcell_edges.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82b3e7b4-a136-4416-a27b-73f45c06d478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copy\n",
    "adata_pat.uns[\"liana_res\"] = res_pat.copy()\n",
    "adata_rand.uns[\"liana_res\"] = res_rand.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c83ded0-9c98-465a-aedf-1190b323b18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dot plot pattern\n",
    "import liana as li\n",
    "\n",
    "li.pl.dotplot(\n",
    "    adata=adata_pat,\n",
    "    colour=\"magnitude_rank\",\n",
    "    size=\"specificity_rank\",\n",
    "    inverse_size=True,\n",
    "    inverse_colour=True,\n",
    "    source_labels=[\"MCF10A\", \"Fibroblast\", \"MCF7\"],\n",
    "    target_labels=[\"MCF7\", \"MCF10A\", \"Fibroblast\"],\n",
    "    top_n=10,\n",
    "    orderby=\"magnitude_rank\",\n",
    "    orderby_ascending=True,\n",
    "    figure_size=(8, 7),\n",
    "    uns_key=\"liana_res\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec0ee7b-24f2-4cce-a42f-d17b2fe419ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dot plot random\n",
    "li.pl.dotplot(\n",
    "    adata=adata_rand,\n",
    "    colour=\"magnitude_rank\",\n",
    "    size=\"specificity_rank\",\n",
    "    inverse_size=True,\n",
    "    inverse_colour=True,\n",
    "    source_labels=[\"MCF10A\", \"Fibroblast\", \"MCF7\"],\n",
    "    target_labels=[\"MCF7\", \"MCF10A\", \"Fibroblast\"],\n",
    "    top_n=10,\n",
    "    orderby=\"magnitude_rank\",\n",
    "    orderby_ascending=True,\n",
    "    figure_size=(8, 7),\n",
    "    uns_key=\"liana_res\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6fb387c-aaf7-4145-b4f6-ac2623b2fef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOVED THE DATA VIZ TO R \n",
    "\n",
    "#\n",
    "\n",
    "#\n",
    "\n",
    "#\n",
    "\n",
    "# Made a global overview dot plot directly comparing Pattern vs Random as the X axis variables\n",
    "# using scPrint2_liana_Pattern_rank_aggregate.csv and scPrint2_liana_Random_rank_aggregate.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3526c8f0-9c96-45a4-b263-83ea14394763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build sender→receiver rewiring table from precomputed edge CSVs \n",
    "import pandas as pd\n",
    "\n",
    "# Load cell–cell edge tables from earlier steps\n",
    "edges_pat = pd.read_csv(\"scPrint2_Pattern_cellcell_edges.csv\")\n",
    "edges_rand = pd.read_csv(\"scPrint2_Random_cellcell_edges.csv\")\n",
    "\n",
    "# Basic sanity checks\n",
    "required_cols = {\"source\", \"target\", \"weight\"}\n",
    "missing_pat = required_cols - set(edges_pat.columns)\n",
    "missing_rand = required_cols - set(edges_rand.columns)\n",
    "\n",
    "if missing_pat:\n",
    "    raise ValueError(\n",
    "        f\"Pattern edges file is missing columns: {sorted(missing_pat)} \"\n",
    "        f\"(has: {list(edges_pat.columns)})\"\n",
    "    )\n",
    "if missing_rand:\n",
    "    raise ValueError(\n",
    "        f\"Random edges file is missing columns: {sorted(missing_rand)} \"\n",
    "        f\"(has: {list(edges_rand.columns)})\"\n",
    "    )\n",
    "\n",
    "print(\"Pattern edges shape:\", edges_pat.shape)\n",
    "print(\"Random edges shape :\", edges_rand.shape)\n",
    "\n",
    "# Merge Pattern and Random edges and compute rewiring\n",
    "df = (\n",
    "    edges_pat.rename(columns={\"weight\": \"weight_pat\"})\n",
    "    .merge(\n",
    "        edges_rand.rename(columns={\"weight\": \"weight_rand\"}),\n",
    "        on=[\"source\", \"target\"],\n",
    "        how=\"outer\",\n",
    "    )\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "# Rewiring metrics\n",
    "df[\"delta\"] = df[\"weight_pat\"] - df[\"weight_rand\"]   # Pattern − Random\n",
    "df[\"abs_delta\"] = df[\"delta\"].abs()\n",
    "\n",
    "# Sort by strongest rewiring\n",
    "df = df.sort_values(\"abs_delta\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"Rewiring table (head):\")\n",
    "print(df.head())\n",
    "\n",
    "# Build plot_df and export\n",
    "plot_df = df.copy()\n",
    "plot_df[\"pair\"] = plot_df[\"source\"].astype(str) + \" → \" + plot_df[\"target\"].astype(str)\n",
    "\n",
    "plot_df.to_csv(\"scPrint2_rewiring_dotplot_data.csv\", index=False)\n",
    "print(\"Saved: scPrint2_rewiring_dotplot_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76276d90-9fb8-4109-9294-e27b0f1a4e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Viz: dot plot of rewiring using plot_df\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load from the CSV we just created\n",
    "plot_df = pd.read_csv(\"scPrint2_rewiring_dotplot_data.csv\")\n",
    "\n",
    "# Order pairs by abs_delta (strongest rewiring first)\n",
    "TOP_K = 30  # adjust if you want more / fewer pairs\n",
    "plot_top = plot_df.sort_values(\"abs_delta\", ascending=False).head(TOP_K).copy()\n",
    "\n",
    "# For plotting, sort by delta so negative/positive are structured along x\n",
    "plot_top = plot_top.sort_values(\"delta\", ascending=True)\n",
    "\n",
    "y = np.arange(len(plot_top))\n",
    "\n",
    "delta = plot_top[\"delta\"].to_numpy()\n",
    "abs_delta = plot_top[\"abs_delta\"].to_numpy()\n",
    "\n",
    "# Dot size scaled by |delta|\n",
    "if abs_delta.max() > 0:\n",
    "    sizes = 150 + 900 * (abs_delta / abs_delta.max())\n",
    "else:\n",
    "    sizes = np.full_like(abs_delta, 300.0)\n",
    "\n",
    "v = np.max(np.abs(delta)) if np.max(np.abs(delta)) > 0 else 1.0\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 0.6 * len(plot_top) + 1.5))\n",
    "\n",
    "sc = ax.scatter(\n",
    "    delta,\n",
    "    y,\n",
    "    s=sizes,\n",
    "    c=delta,\n",
    "    cmap=\"coolwarm\",\n",
    "    vmin=-v,\n",
    "    vmax=v,\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=0.6,\n",
    "    alpha=0.9,\n",
    ")\n",
    "\n",
    "ax.axvline(0, linestyle=\"--\", linewidth=1)\n",
    "\n",
    "ax.set_yticks(y)\n",
    "ax.set_yticklabels(plot_top[\"pair\"], fontsize=11)\n",
    "\n",
    "ax.set_xlabel(\"Δ LR pair count (Pattern − Random)\", fontsize=12)\n",
    "ax.set_title(\n",
    "    \"Sender → Receiver signaling rewiring\\n\"\n",
    "    \"(edge LR counts identical to circos plot)\",\n",
    "    fontsize=13,\n",
    ")\n",
    "\n",
    "ax.grid(True, axis=\"x\", linestyle=\":\", linewidth=0.8)\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "cbar = plt.colorbar(sc, ax=ax, pad=0.02)\n",
    "cbar.set_label(\"Δ LR pair count (Pattern − Random)\", rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"scPrint2_rewiring_dotplot_MATCHES_CIRCOS.png\",\n",
    "            dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved: scPrint2_rewiring_dotplot_MATCHES_CIRCOS.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a686289e-07f9-4c95-bba7-3c171ae7a870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOVED THE DATA VIZ TO R \n",
    "\n",
    "#\n",
    "\n",
    "#\n",
    "\n",
    "#\n",
    "\n",
    "# Made Pairwise rewiring plot using scPrint2_rewiring_dotplot_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14010ac6-8d17-4ef4-ab35-14fd19f84cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF / PROGENy activity and OmniPath LR resource (for Sankey plot)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import decoupler as dc\n",
    "import scanpy as sc\n",
    "import omnipath as op\n",
    "\n",
    "# Helper: make a log1p matrix for decoupler (from counts)\n",
    "def prep_log1p_from_counts(ad_in, layer=\"counts\", target_sum=1e4):\n",
    "    ad = ad_in.copy()\n",
    "    X = ad.layers[layer] if layer in ad.layers else ad.X\n",
    "    ad.X = X.copy()\n",
    "    sc.pp.normalize_total(ad, target_sum=target_sum)\n",
    "    sc.pp.log1p(ad)\n",
    "    return ad\n",
    "\n",
    "# Helper: build expression df for decoupler (cells x genes)\n",
    "# Only keeps genes that appear in the network targets\n",
    "def expr_df_for_net(ad, net, target_col=\"target\"):\n",
    "    genes = pd.Index(ad.var_names.astype(str))\n",
    "    net_genes = pd.Index(pd.unique(net[target_col].astype(str)))\n",
    "    use_genes = genes.intersection(net_genes)\n",
    "\n",
    "    if len(use_genes) == 0:\n",
    "        raise ValueError(\n",
    "            \"No overlap between adata.var_names and network target genes.\\n\"\n",
    "            f\"Example adata genes: {genes[:5].tolist()}\\n\"\n",
    "            f\"Example net targets: {net_genes[:5].tolist()}\"\n",
    "        )\n",
    "\n",
    "    sub = ad[:, use_genes].copy()\n",
    "    X = sub.X\n",
    "    if hasattr(X, \"toarray\"):\n",
    "        X = X.toarray()\n",
    "    X = np.asarray(X)\n",
    "\n",
    "    return pd.DataFrame(X, index=sub.obs_names.astype(str), columns=use_genes.astype(str))\n",
    "\n",
    "# Helper: run decoupler method and return (est_df, pval_df_or_None)\n",
    "def run_decoupler_safe(method_fn, *, data, net, **kwargs):\n",
    "    out = method_fn(data=data, net=net, **kwargs)\n",
    "\n",
    "    if out is None:\n",
    "        raise RuntimeError(\n",
    "            f\"{method_fn.__name__} returned None. \"\n",
    "            \"This usually means the method crashed internally or produced no results.\"\n",
    "        )\n",
    "\n",
    "    if isinstance(out, tuple) and len(out) == 2:\n",
    "        est, pvals = out\n",
    "        return est, pvals\n",
    "\n",
    "    if isinstance(out, pd.DataFrame):\n",
    "        return out, None\n",
    "\n",
    "    if isinstance(out, dict):\n",
    "        est = out.get(\"estimate\", out.get(\"estimates\", None))\n",
    "        pvs = out.get(\"pvals\", out.get(\"p_values\", None))\n",
    "        if est is None:\n",
    "            raise RuntimeError(\n",
    "                f\"{method_fn.__name__} returned dict without estimates keys: {list(out.keys())}\"\n",
    "            )\n",
    "        return est, pvs\n",
    "\n",
    "    raise RuntimeError(f\"Unexpected output type from {method_fn.__name__}: {type(out)}\")\n",
    "\n",
    "# Helper: align decoupler estimates to AnnData obs\n",
    "def store_activity_in_obsm(ad, est_df, obsm_key, uns_cols_key):\n",
    "    if not isinstance(est_df, pd.DataFrame):\n",
    "        est_df = pd.DataFrame(np.asarray(est_df))\n",
    "\n",
    "    est_df.index = est_df.index.astype(str)\n",
    "    ad_idx = ad.obs_names.astype(str)\n",
    "\n",
    "    est_aligned = est_df.reindex(ad_idx)\n",
    "\n",
    "    n_missing = int(est_aligned.isna().all(axis=1).sum())\n",
    "    if n_missing > 0:\n",
    "        print(\n",
    "            f\"WARNING: {obsm_key}: {n_missing} cells missing in decoupler output. \"\n",
    "            \"Filling NaNs with 0.\"\n",
    "        )\n",
    "        est_aligned = est_aligned.fillna(0)\n",
    "\n",
    "    ad.obsm[obsm_key] = est_aligned.to_numpy()\n",
    "    ad.uns[uns_cols_key] = est_aligned.columns.astype(str).tolist()\n",
    "\n",
    "# Receiver-only objects (MCF7)\n",
    "TARGET_CT = \"MCF7\"\n",
    "\n",
    "adata_cc.var_names_make_unique()\n",
    "adata_mcf7 = adata_cc[adata_cc.obs[\"CellType\"].astype(str) == TARGET_CT].copy()\n",
    "\n",
    "adata_mcf7_pat  = prep_log1p_from_counts(\n",
    "    adata_mcf7[adata_mcf7.obs[\"condition\"] == \"Pattern\"].copy(),\n",
    "    layer=\"counts\"\n",
    ")\n",
    "adata_mcf7_rand = prep_log1p_from_counts(\n",
    "    adata_mcf7[adata_mcf7.obs[\"condition\"] == \"Random\"].copy(),\n",
    "    layer=\"counts\"\n",
    ")\n",
    "\n",
    "# Load shared resources\n",
    "# CollecTRI TF network\n",
    "def load_collectri_net():\n",
    "    # Prefer new API if available\n",
    "    if hasattr(dc, \"get_collectri\"):\n",
    "        return dc.get_collectri(organism=\"human\", split_complexes=True)\n",
    "    # Fallback to old API\n",
    "    if hasattr(dc, \"op\") and hasattr(dc.op, \"collectri\"):\n",
    "        return dc.op.collectri(\n",
    "            organism=\"human\",\n",
    "            remove_complexes=False,\n",
    "            license=\"academic\",\n",
    "            verbose=False,\n",
    "        )\n",
    "    raise RuntimeError(\"No CollecTRI loader found in this decoupler version.\")\n",
    "\n",
    "# PROGENy pathway network\n",
    "def load_progeny_net():\n",
    "    if hasattr(dc, \"get_progeny\"):\n",
    "        return dc.get_progeny(organism=\"human\", top=100)\n",
    "    if hasattr(dc, \"op\") and hasattr(dc.op, \"progeny\"):\n",
    "        return dc.op.progeny(organism=\"human\", top=100, verbose=False)\n",
    "    raise RuntimeError(\"No PROGENy loader found in this decoupler version.\")\n",
    "\n",
    "tf_net = load_collectri_net()\n",
    "progeny_net = load_progeny_net()\n",
    "\n",
    "print(\"TF net shape:\", tf_net.shape)\n",
    "print(\"PROGENy net shape:\", progeny_net.shape)\n",
    "\n",
    "# OmniPath ligand–receptor resource (for LR categories)\n",
    "try:\n",
    "    omni_lr = op.interactions.import_intercell_network(\n",
    "        genesymbols_only=True\n",
    "    )\n",
    "except TypeError:\n",
    "    print(\n",
    "        \"import_intercell_network does not accept `genesymbols_only`; \"\n",
    "        \"calling without arguments and will handle symbols downstream.\"\n",
    "    )\n",
    "    omni_lr = op.interactions.import_intercell_network()\n",
    "\n",
    "print(\"OmniPath LR table shape:\", omni_lr.shape)\n",
    "\n",
    "# TF activity (CollecTRI) via ULM\n",
    "X_pat_tf = expr_df_for_net(adata_mcf7_pat, tf_net, target_col=\"target\")\n",
    "X_ran_tf = expr_df_for_net(adata_mcf7_rand, tf_net, target_col=\"target\")\n",
    "\n",
    "tf_est_pat, tf_p_pat   = run_decoupler_safe(dc.mt.ulm, data=X_pat_tf, net=tf_net, tmin=5, verbose=False)\n",
    "tf_est_rand, tf_p_rand = run_decoupler_safe(dc.mt.ulm, data=X_ran_tf, net=tf_net, tmin=5, verbose=False)\n",
    "\n",
    "store_activity_in_obsm(adata_mcf7_pat,  tf_est_pat,  obsm_key=\"tf_ulm\", uns_cols_key=\"tf_ulm_cols\")\n",
    "store_activity_in_obsm(adata_mcf7_rand, tf_est_rand, obsm_key=\"tf_ulm\", uns_cols_key=\"tf_ulm_cols\")\n",
    "\n",
    "print(\"TF ULM shapes:\", adata_mcf7_pat.obsm[\"tf_ulm\"].shape, adata_mcf7_rand.obsm[\"tf_ulm\"].shape)\n",
    "\n",
    "\n",
    "# Pathway activity (PROGENy) via MLM\n",
    "X_pat_pw = expr_df_for_net(adata_mcf7_pat, progeny_net, target_col=\"target\")\n",
    "X_ran_pw = expr_df_for_net(adata_mcf7_rand, progeny_net, target_col=\"target\")\n",
    "\n",
    "pw_est_pat, pw_p_pat   = run_decoupler_safe(dc.mt.mlm, data=X_pat_pw, net=progeny_net, verbose=False)\n",
    "pw_est_rand, pw_p_rand = run_decoupler_safe(dc.mt.mlm, data=X_ran_pw, net=progeny_net, verbose=False)\n",
    "\n",
    "store_activity_in_obsm(adata_mcf7_pat,  pw_est_pat,  obsm_key=\"progeny\", uns_cols_key=\"progeny_cols\")\n",
    "store_activity_in_obsm(adata_mcf7_rand, pw_est_rand, obsm_key=\"progeny\", uns_cols_key=\"progeny_cols\")\n",
    "\n",
    "print(\"PROGENy shapes:\", adata_mcf7_pat.obsm[\"progeny\"].shape, adata_mcf7_rand.obsm[\"progeny\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4ff010-b430-491c-9c12-382cac8ddac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sankey Plot with globally comparable edge widths\n",
    "# LIANA LR (MCF10A→MCF7) → TF(ULM) → PROGENy, Pattern vs Random\n",
    "# - Edge thicknesses are globally normalized across both conditions, per band\n",
    "# - Node order in each band is shared and alphabetical across both conditions\n",
    "# - Ligand→Receptor edges are gray\n",
    "# - All other edges are colored based on spearman (red = positive, blue = negative)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import decoupler as dc\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Select Sender -> Receiver and set how many nodes and edges to show\n",
    "SOURCE_CT = \"MCF10A\"\n",
    "TARGET_CT = \"MCF7\"\n",
    "\n",
    "TOP_LR_LIGANDS   = 10\n",
    "TOP_LR_RECEPTORS = 10\n",
    "TOP_TF_NODES     = 10\n",
    "TOP_PW_NODES     = 15 # This is the entire progeny set\n",
    "\n",
    "TOP_REC_TO_TF_EDGES = 5\n",
    "TOP_TF_TO_PW_EDGES  = 5\n",
    "\n",
    "OUT_PREFIX = \"MCF10A_to_MCF7_Sankey_4band\"\n",
    "# ------------------------------------------------------\n",
    "\n",
    "# Choose LIANA score column if needed\n",
    "if \"score_col_pat\" not in globals() or \"score_col_rand\" not in globals():\n",
    "    preferred_cols = [\n",
    "        \"magnitude_rank\",\n",
    "        \"magnitude\",\n",
    "        \"lr_score\",\n",
    "        \"score\",\n",
    "        \"pvalue\",\n",
    "        \"p_adj\",\n",
    "        \"p_adj_rank\",\n",
    "    ]\n",
    "    chosen = None\n",
    "    for c in preferred_cols:\n",
    "        if c in res_pat.columns:\n",
    "            chosen = c\n",
    "            break\n",
    "\n",
    "    if chosen is None:\n",
    "        numeric_cols = res_pat.select_dtypes(include=\"number\").columns.tolist()\n",
    "        if not numeric_cols:\n",
    "            raise ValueError(\n",
    "                \"Could not auto detect a numeric LIANA score column in `res_pat`.\\n\"\n",
    "                \"Inspect `res_pat.columns` and set `score_col_pat` and `score_col_rand` manually.\"\n",
    "            )\n",
    "        chosen = numeric_cols[0]\n",
    "\n",
    "    score_col_pat = chosen\n",
    "    score_col_rand = chosen\n",
    "    print(f\"Auto selected LIANA score column: {chosen}\")\n",
    "\n",
    "# Helpers\n",
    "def _to_dense(X):\n",
    "    if hasattr(X, \"toarray\"):\n",
    "        X = X.toarray()\n",
    "    return np.asarray(X)\n",
    "\n",
    "def prep_log1p_from_counts(ad_in, layer=\"counts\", target_sum=1e4):\n",
    "    ad = ad_in.copy()\n",
    "    X = ad.layers[layer] if layer in ad.layers else ad.X\n",
    "    ad.X = X.copy()\n",
    "    sc.pp.normalize_total(ad, target_sum=target_sum)\n",
    "    sc.pp.log1p(ad)\n",
    "    return ad\n",
    "\n",
    "def split_lr_first_gene(ligand_complex, receptor_complex):\n",
    "    lig = str(ligand_complex).split(\"_\")[0]\n",
    "    rec = str(receptor_complex).split(\"_\")[0]\n",
    "    return lig, rec\n",
    "\n",
    "def liana_lr_gene_level(res_df, score_col, source_ct, target_ct):\n",
    "    \"\"\"\n",
    "    Gene level LR table for sender→receiver:\n",
    "      lig, rec, lr_score, n_pairs\n",
    "    Aggregated over complexes that map to the same (lig, rec).\n",
    "    \"\"\"\n",
    "    df = res_df.copy()\n",
    "    df = df[(df[\"source\"] == source_ct) & (df[\"target\"] == target_ct)].copy()\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=[\"lig\", \"rec\", \"lr_score\", \"n_pairs\"])\n",
    "\n",
    "    df[score_col] = pd.to_numeric(df[score_col], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[score_col])\n",
    "\n",
    "    # flip rank type scores so higher = stronger\n",
    "    is_rank = \"rank\" in str(score_col).lower()\n",
    "    if is_rank:\n",
    "        df[\"_strength\"] = 1.0 / (df[score_col].astype(float) + 1e-9)\n",
    "    else:\n",
    "        df[\"_strength\"] = df[score_col].astype(float)\n",
    "\n",
    "    lig_rec = df.apply(\n",
    "        lambda r: split_lr_first_gene(r[\"ligand_complex\"], r[\"receptor_complex\"]),\n",
    "        axis=1,\n",
    "    )\n",
    "    df[\"lig\"] = [x[0] for x in lig_rec]\n",
    "    df[\"rec\"] = [x[1] for x in lig_rec]\n",
    "\n",
    "    agg = (\n",
    "        df.groupby([\"lig\", \"rec\"], as_index=False)\n",
    "          .agg(lr_score=(\"_strength\", \"mean\"),\n",
    "               n_pairs=(\"_strength\", \"size\"))\n",
    "          .sort_values(\"lr_score\", ascending=False)\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "    return agg\n",
    "\n",
    "def expr_df_for_genes(ad, genes):\n",
    "    genes = pd.Index([str(g) for g in genes])\n",
    "    use = pd.Index(ad.var_names.astype(str)).intersection(genes)\n",
    "    sub = ad[:, use].copy()\n",
    "    X = _to_dense(sub.X)\n",
    "    return pd.DataFrame(X, index=sub.obs_names.astype(str), columns=use.astype(str))\n",
    "\n",
    "def expr_df_for_net(ad, net, target_col=\"target\"):\n",
    "    genes     = pd.Index(ad.var_names.astype(str))\n",
    "    net_genes = pd.Index(pd.unique(net[target_col].astype(str)))\n",
    "    use_genes = genes.intersection(net_genes)\n",
    "    if len(use_genes) == 0:\n",
    "        raise ValueError(\"No overlap between adata.var_names and network target genes.\")\n",
    "    sub = ad[:, use_genes].copy()\n",
    "    X = _to_dense(sub.X)\n",
    "    return pd.DataFrame(X, index=sub.obs_names.astype(str), columns=use_genes.astype(str))\n",
    "\n",
    "def spearman_corr(A_df, B_df):\n",
    "    \"\"\"\n",
    "    Column wise Spearman correlation between A (cells × genesA)\n",
    "    and B (cells × genesB).\n",
    "    Returns DataFrame with index = A.columns, columns = B.columns.\n",
    "    \"\"\"\n",
    "    idx = A_df.index.intersection(B_df.index)\n",
    "    if len(idx) == 0:\n",
    "        raise ValueError(\"No overlapping cells for correlation.\")\n",
    "    A = A_df.loc[idx]\n",
    "    B = B_df.loc[idx]\n",
    "\n",
    "    # rank per column\n",
    "    Ar = A.rank(axis=0, method=\"average\").to_numpy(dtype=float)\n",
    "    Br = B.rank(axis=0, method=\"average\").to_numpy(dtype=float)\n",
    "\n",
    "    # z-score the ranks for each column\n",
    "    Ar = (Ar - Ar.mean(axis=0, keepdims=True)) / (\n",
    "        Ar.std(axis=0, ddof=0, keepdims=True) + 1e-12\n",
    "    )\n",
    "    Br = (Br - Br.mean(axis=0, keepdims=True)) / (\n",
    "        Br.std(axis=0, ddof=0, keepdims=True) + 1e-12\n",
    "    )\n",
    "\n",
    "    n = Ar.shape[0]\n",
    "    C = (Ar.T @ Br) / max(n, 1)\n",
    "\n",
    "    return pd.DataFrame(C, index=A.columns, columns=B.columns)\n",
    "\n",
    "\n",
    "# OmniPath LR annotation helper - for categorizing LR pairs\n",
    "possible_source_cols = [\n",
    "    \"genesymbol_intercell_source\",\n",
    "    \"source_genesymbol\",\n",
    "    \"source_genesymbols\",\n",
    "    \"source_name\",\n",
    "    \"source\",\n",
    "]\n",
    "possible_target_cols = [\n",
    "    \"genesymbol_intercell_target\",\n",
    "    \"target_genesymbol\",\n",
    "    \"target_genesymbols\",\n",
    "    \"target_name\",\n",
    "    \"target\",\n",
    "]\n",
    "\n",
    "def _pick_col(df, candidates, label):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    raise KeyError(\n",
    "        f\"No suitable {label} column found in omni_lr. \"\n",
    "        f\"Available columns: {list(df.columns)}\"\n",
    "    )\n",
    "\n",
    "omni_source_col = _pick_col(omni_lr, possible_source_cols, \"source gene symbol\")\n",
    "omni_target_col = _pick_col(omni_lr, possible_target_cols, \"target gene symbol\")\n",
    "\n",
    "lig_cat_col = \"category_intercell_source\" if \"category_intercell_source\" in omni_lr.columns else None\n",
    "rec_cat_col = \"category_intercell_target\" if \"category_intercell_target\" in omni_lr.columns else None\n",
    "\n",
    "print(\n",
    "    f\"Using OmniPath columns: \"\n",
    "    f\"source={omni_source_col}, target={omni_target_col}, \"\n",
    "    f\"lig_cat={lig_cat_col}, rec_cat={rec_cat_col}\"\n",
    ")\n",
    "\n",
    "def _lr_category_from_row(row):\n",
    "    labels = []\n",
    "    if lig_cat_col is not None and pd.notna(row[lig_cat_col]):\n",
    "        labels.append(str(row[lig_cat_col]))\n",
    "    if rec_cat_col is not None and pd.notna(row[rec_cat_col]):\n",
    "        labels.append(str(row[rec_cat_col]))\n",
    "    if not labels:\n",
    "        return \"Not annotated (OmniPath)\"\n",
    "    return \"→\".join(labels)\n",
    "\n",
    "omni_lr_pairs = (\n",
    "    omni_lr\n",
    "    .assign(\n",
    "        lig    = omni_lr[omni_source_col].astype(str),\n",
    "        rec    = omni_lr[omni_target_col].astype(str),\n",
    "        lr_cat = omni_lr.apply(_lr_category_from_row, axis=1),\n",
    "    )\n",
    "    .groupby([\"lig\", \"rec\"])[\"lr_cat\"]\n",
    "    .apply(lambda s: \"|\".join(sorted(pd.unique(s.astype(str)))))\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "def annotate_lr_category(lig, rec):\n",
    "    return omni_lr_pairs.get((str(lig), str(rec)), \"Not annotated (OmniPath)\")\n",
    "\n",
    "\n",
    "# Compute edges for a single condition (no width scaling yet)\n",
    "def compute_edges_for_condition(condition_label, res_df, score_col, adata_cc,\n",
    "                                tf_net, progeny_net):\n",
    "    # ----- 1. LIANA LR -----\n",
    "    lr_agg = liana_lr_gene_level(res_df, score_col, SOURCE_CT, TARGET_CT)\n",
    "    if lr_agg.empty:\n",
    "        raise ValueError(f\"No LIANA interactions for {SOURCE_CT}→{TARGET_CT} in {condition_label}.\")\n",
    "\n",
    "    top_lr = lr_agg.sort_values(\"lr_score\", ascending=False).copy()\n",
    "\n",
    "    top_recs = (\n",
    "        top_lr.groupby(\"rec\", as_index=False)[\"lr_score\"].max()\n",
    "              .sort_values(\"lr_score\", ascending=False)\n",
    "              .head(TOP_LR_RECEPTORS)[\"rec\"]\n",
    "              .tolist()\n",
    "    )\n",
    "\n",
    "    top_ligs = (\n",
    "        top_lr[top_lr[\"rec\"].isin(top_recs)]\n",
    "        .groupby(\"lig\", as_index=False)[\"lr_score\"].max()\n",
    "        .sort_values(\"lr_score\", ascending=False)\n",
    "        .head(TOP_LR_LIGANDS)[\"lig\"]\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "    lr_edges = top_lr[top_lr[\"rec\"].isin(top_recs) & top_lr[\"lig\"].isin(top_ligs)].copy()\n",
    "    if lr_edges.empty:\n",
    "        top_ligs = (\n",
    "            top_lr.groupby(\"lig\", as_index=False)[\"lr_score\"].max()\n",
    "                  .sort_values(\"lr_score\", ascending=False)\n",
    "                  .head(TOP_LR_LIGANDS)[\"lig\"]\n",
    "                  .tolist()\n",
    "        )\n",
    "        lr_edges = top_lr[top_lr[\"rec\"].isin(top_recs) & top_lr[\"lig\"].isin(top_ligs)].copy()\n",
    "\n",
    "    # OmniPath category for hover and export (edges will be gray)\n",
    "    lr_edges[\"omnipath_category\"] = [\n",
    "        annotate_lr_category(l, r) for l, r in zip(lr_edges[\"lig\"], lr_edges[\"rec\"])\n",
    "    ]\n",
    "\n",
    "    # ----- 2. Receiver cells (MCF7) expression -----\n",
    "    ad_rec = adata_cc[\n",
    "        (adata_cc.obs[\"condition\"].astype(str) == condition_label) &\n",
    "        (adata_cc.obs[\"CellType\"].astype(str) == TARGET_CT)\n",
    "    ].copy()\n",
    "    if ad_rec.n_obs == 0:\n",
    "        raise ValueError(f\"No receiver cells for condition={condition_label}, CellType={TARGET_CT}.\")\n",
    "\n",
    "    ad_rec_log = prep_log1p_from_counts(ad_rec, layer=\"counts\")\n",
    "\n",
    "    rec_expr = expr_df_for_genes(ad_rec_log, top_recs)\n",
    "    rec_present = rec_expr.columns.tolist()\n",
    "    if len(rec_present) == 0:\n",
    "        raise ValueError(\n",
    "            f\"None of the selected receptors are in adata.var_names for {condition_label}.\\n\"\n",
    "            f\"Selected receptors were: {top_recs}\"\n",
    "        )\n",
    "\n",
    "    # ----- 3. TF activity (CollecTRI ULM) -----\n",
    "    X_tf = expr_df_for_net(ad_rec_log, tf_net, target_col=\"target\")\n",
    "    tf_est = dc.mt.ulm(data=X_tf, net=tf_net, tmin=5, verbose=False)\n",
    "    if tf_est is None:\n",
    "        raise RuntimeError(\"decoupler dc.mt.ulm returned None.\")\n",
    "    if isinstance(tf_est, tuple):\n",
    "        tf_est = tf_est[0]\n",
    "\n",
    "    idx = ad_rec_log.obs_names.astype(str)\n",
    "    tf_est = tf_est.reindex(idx, fill_value=0)\n",
    "\n",
    "    tf_top_series = tf_est.mean(axis=0).sort_values(ascending=False).head(max(TOP_TF_NODES, 10))\n",
    "    tf_candidates = tf_top_series.index.tolist()\n",
    "\n",
    "    corr_rec_tf = spearman_corr(rec_expr, tf_est[tf_candidates])\n",
    "\n",
    "    tf_strength = corr_rec_tf.abs().max(axis=0).sort_values(ascending=False)\n",
    "    tf_nodes = tf_strength.head(TOP_TF_NODES).index.tolist()\n",
    "\n",
    "    rec_tf_edges_list = []\n",
    "    for r in rec_present:\n",
    "        row = corr_rec_tf.loc[r, tf_nodes].copy()\n",
    "        top_tf = row.abs().sort_values(ascending=False).head(TOP_REC_TO_TF_EDGES).index.tolist()\n",
    "        for t in top_tf:\n",
    "            rec_tf_edges_list.append((r, t, float(row[t])))\n",
    "\n",
    "    rec_tf_edges = pd.DataFrame(rec_tf_edges_list, columns=[\"receptor\", \"TF\", \"spearman_r\"])\n",
    "\n",
    "    # ----- 4. PROGENy MLM -----\n",
    "    X_pw = expr_df_for_net(ad_rec_log, progeny_net, target_col=\"target\")\n",
    "    pw_est = dc.mt.mlm(data=X_pw, net=progeny_net, verbose=False)\n",
    "    if pw_est is None:\n",
    "        raise RuntimeError(\"decoupler dc.mt.mlm returned None.\")\n",
    "    if isinstance(pw_est, tuple):\n",
    "        pw_est = pw_est[0]\n",
    "\n",
    "    pw_est = pw_est.reindex(idx, fill_value=0)\n",
    "\n",
    "    pw_top_series = pw_est.mean(axis=0).sort_values(ascending=False).head(max(TOP_PW_NODES, 10))\n",
    "    pw_candidates = pw_top_series.index.tolist()\n",
    "\n",
    "    corr_tf_pw = spearman_corr(tf_est[tf_nodes], pw_est[pw_candidates])\n",
    "\n",
    "    pw_strength = corr_tf_pw.abs().max(axis=0).sort_values(ascending=False)\n",
    "    pw_nodes = pw_strength.head(TOP_PW_NODES).index.tolist()\n",
    "\n",
    "    tf_pw_edges_list = []\n",
    "    for t in tf_nodes:\n",
    "        row = corr_tf_pw.loc[t, pw_nodes].copy()\n",
    "        top_pw = row.abs().sort_values(ascending=False).head(TOP_TF_TO_PW_EDGES).index.tolist()\n",
    "        for p in top_pw:\n",
    "            tf_pw_edges_list.append((t, p, float(row[p])))\n",
    "\n",
    "    tf_pw_edges = pd.DataFrame(tf_pw_edges_list, columns=[\"TF\", \"pathway\", \"spearman_r\"])\n",
    "\n",
    "    return lr_edges, rec_tf_edges, tf_pw_edges\n",
    "\n",
    "# Compute edges for Pattern and Random\n",
    "adata_cc.var_names_make_unique()\n",
    "\n",
    "lr_pat_edges, rec_tf_pat, tf_pw_pat = compute_edges_for_condition(\n",
    "    \"Pattern\", res_pat, score_col_pat, adata_cc, tf_net, progeny_net\n",
    ")\n",
    "lr_ran_edges, rec_tf_ran, tf_pw_ran = compute_edges_for_condition(\n",
    "    \"Random\", res_rand, score_col_rand, adata_cc, tf_net, progeny_net\n",
    ")\n",
    "\n",
    "# 3) Global scaling helper: maps to [min_width, max_width], using a high quantile to avoid single-edge domination\n",
    "def normalize_pair(\n",
    "    pat_df,\n",
    "    ran_df,\n",
    "    col,\n",
    "    min_width=10.0,        \n",
    "    max_width=20.0,       \n",
    "    use_abs=False,\n",
    "    vmax_quantile=0.9,   \n",
    "):\n",
    "    \"\"\"\n",
    "    Take Pattern + Random dataframes, look at (abs) col across both,\n",
    "    and map to a common [min_width, max_width] range.\n",
    "    Uses a high quantile instead of the absolute max so that a single\n",
    "    huge edge doesn't compress all other edges.\n",
    "    \"\"\"\n",
    "    pat_vals = pat_df[col].astype(float)\n",
    "    ran_vals = ran_df[col].astype(float)\n",
    "\n",
    "    if use_abs:\n",
    "        pat_vals = pat_vals.abs()\n",
    "        ran_vals = ran_vals.abs()\n",
    "\n",
    "    all_vals = pd.concat([pat_vals, ran_vals])\n",
    "\n",
    "    # high quantile as effective vmax, then clip anything above\n",
    "    vmax = all_vals.quantile(vmax_quantile)\n",
    "    if vmax <= 0 or np.isnan(vmax):\n",
    "        # fallback: everything same width\n",
    "        mid = (min_width + max_width) / 2.0\n",
    "        return (\n",
    "            pd.Series(np.full(len(pat_df), mid), index=pat_df.index),\n",
    "            pd.Series(np.full(len(ran_df), mid), index=ran_df.index),\n",
    "        )\n",
    "\n",
    "    # clip to vmax so outliers don't dominate\n",
    "    pat_clipped = pat_vals.clip(lower=0, upper=vmax)\n",
    "    ran_clipped = ran_vals.clip(lower=0, upper=vmax)\n",
    "\n",
    "    # scale 0..1\n",
    "    pat_norm = pat_clipped / vmax\n",
    "    ran_norm = ran_clipped / vmax\n",
    "\n",
    "    pat_w = min_width + (max_width - min_width) * pat_norm\n",
    "    ran_w = min_width + (max_width - min_width) * ran_norm\n",
    "\n",
    "    return pat_w, ran_w\n",
    "\n",
    "# LR band: use lr_score (already flipped so higher = stronger)\n",
    "lr_pat_w, lr_ran_w = normalize_pair(\n",
    "    lr_pat_edges,\n",
    "    lr_ran_edges,\n",
    "    col=\"lr_score\",\n",
    "    use_abs=False,\n",
    ")\n",
    "lr_pat_edges[\"value\"]  = lr_pat_w\n",
    "lr_ran_edges[\"value\"]  = lr_ran_w\n",
    "# also store as w_plot for CSV / R plotting if desired\n",
    "lr_pat_edges[\"w_plot\"] = lr_pat_w\n",
    "lr_ran_edges[\"w_plot\"] = lr_ran_w\n",
    "\n",
    "# Rec→TF: use |Spearman|\n",
    "rec_tf_pat_w, rec_tf_ran_w = normalize_pair(\n",
    "    rec_tf_pat,\n",
    "    rec_tf_ran,\n",
    "    col=\"spearman_r\",\n",
    "    use_abs=True,\n",
    ")\n",
    "rec_tf_pat[\"value\"] = rec_tf_pat_w\n",
    "rec_tf_ran[\"value\"] = rec_tf_ran_w\n",
    "\n",
    "# TF→PW: use |Spearman|\n",
    "tf_pw_pat_w, tf_pw_ran_w = normalize_pair(\n",
    "    tf_pw_pat,\n",
    "    tf_pw_ran,\n",
    "    col=\"spearman_r\",\n",
    "    use_abs=True,\n",
    ")\n",
    "tf_pw_pat[\"value\"] = tf_pw_pat_w\n",
    "tf_pw_ran[\"value\"] = tf_pw_ran_w\n",
    "\n",
    "print(\"LR Pattern width range:\", lr_pat_edges[\"value\"].min(), lr_pat_edges[\"value\"].max())\n",
    "print(\"LR Random width range:\", lr_ran_edges[\"value\"].min(), lr_ran_edges[\"value\"].max())\n",
    "print(\"Rec–TF Pattern width range:\", rec_tf_pat[\"value\"].min(), rec_tf_pat[\"value\"].max())\n",
    "print(\"Rec–TF Random width range:\", rec_tf_ran[\"value\"].min(), rec_tf_ran[\"value\"].max())\n",
    "print(\"TF–PW Pattern width range:\", tf_pw_pat[\"value\"].min(), tf_pw_pat[\"value\"].max())\n",
    "print(\"TF–PW Random width range:\", tf_pw_ran[\"value\"].min(), tf_pw_ran[\"value\"].max())\n",
    "\n",
    "# 4) Global, case insensitive alphabetical node orders\n",
    "lig_global = sorted(\n",
    "    set(lr_pat_edges[\"lig\"].unique()) | set(lr_ran_edges[\"lig\"].unique()),\n",
    "    key=lambda x: x.upper(),\n",
    ")\n",
    "rec_global = sorted(\n",
    "    set(lr_pat_edges[\"rec\"].unique()) | set(lr_ran_edges[\"rec\"].unique()),\n",
    "    key=lambda x: x.upper(),\n",
    ")\n",
    "tf_global = sorted(\n",
    "    set(rec_tf_pat[\"TF\"].unique()) | set(rec_tf_ran[\"TF\"].unique()),\n",
    "    key=lambda x: x.upper(),\n",
    ")\n",
    "pw_global = sorted(\n",
    "    set(tf_pw_pat[\"pathway\"].unique()) | set(tf_pw_ran[\"pathway\"].unique()),\n",
    "    key=lambda x: x.upper(),\n",
    ")\n",
    "\n",
    "# 5) Sankey figure builder using pre-computed 'value' widths\n",
    "def make_sankey_from_edges(condition_label,\n",
    "                           lr_edges, rec_tf_edges, tf_pw_edges,\n",
    "                           lig_order, rec_order, tf_order, pw_order):\n",
    "\n",
    "    # enforce shared global alphabetical order per band\n",
    "    lig_set = set(lr_edges[\"lig\"])\n",
    "    rec_set = set(lr_edges[\"rec\"])\n",
    "    tf_set  = set(rec_tf_edges[\"TF\"])\n",
    "    pw_set  = set(tf_pw_edges[\"pathway\"])\n",
    "\n",
    "    lig_list = [g for g in lig_order if g in lig_set]\n",
    "    rec_list = [g for g in rec_order if g in rec_set]\n",
    "    tf_list  = [g for g in tf_order  if g in tf_set]\n",
    "    pw_list  = [g for g in pw_order  if g in pw_set]\n",
    "\n",
    "    lig_nodes = [f\"LIG:{x}\" for x in lig_list]\n",
    "    rec_nodes = [f\"REC:{x}\" for x in rec_list]\n",
    "    tf_nodes2 = [f\"TF:{x}\"  for x in tf_list]\n",
    "    pw_nodes2 = [f\"PW:{x}\"  for x in pw_list]\n",
    "\n",
    "    all_nodes  = lig_nodes + rec_nodes + tf_nodes2 + pw_nodes2\n",
    "    node_index = {n: i for i, n in enumerate(all_nodes)}\n",
    "\n",
    "    # x positions (four vertical bands)\n",
    "    x_lig = 0.02\n",
    "    x_rec = 0.34\n",
    "    x_tf  = 0.66\n",
    "    x_pw  = 0.98\n",
    "\n",
    "    def _linspace_y(n):\n",
    "        if n <= 1:\n",
    "            return [0.5]\n",
    "        return np.linspace(0.02, 0.98, n).tolist()\n",
    "\n",
    "    y_lig = _linspace_y(len(lig_nodes))\n",
    "    y_rec = _linspace_y(len(rec_nodes))\n",
    "    y_tf  = _linspace_y(len(tf_nodes2))\n",
    "    y_pw  = _linspace_y(len(pw_nodes2))\n",
    "\n",
    "    node_x = (\n",
    "        [x_lig]*len(lig_nodes) +\n",
    "        [x_rec]*len(rec_nodes) +\n",
    "        [x_tf]*len(tf_nodes2) +\n",
    "        [x_pw]*len(pw_nodes2)\n",
    "    )\n",
    "    node_y = y_lig + y_rec + y_tf + y_pw\n",
    "\n",
    "    # node colors\n",
    "    node_colors = (\n",
    "        [\"#D15B5D\"]*len(lig_nodes) +   # ligands\n",
    "        [\"#D795D1\"]*len(rec_nodes) +   # receptors\n",
    "        [\"#FFDC58\"]*len(tf_nodes2) +   # TFs\n",
    "        [\"#44546A\"]*len(pw_nodes2)     # pathways\n",
    "    )\n",
    "\n",
    "    sources = []\n",
    "    targets = []\n",
    "    values  = []\n",
    "    colors  = []\n",
    "    hover   = []\n",
    "\n",
    "    # Ligand → Receptor edges (gray, uses lr_edges['value'])\n",
    "    for _, r in lr_edges.iterrows():\n",
    "        lig = f\"LIG:{r['lig']}\"\n",
    "        rec = f\"REC:{r['rec']}\"\n",
    "        if lig not in node_index or rec not in node_index:\n",
    "            continue\n",
    "\n",
    "        w = float(r[\"value\"])\n",
    "        if w <= 0:\n",
    "            continue\n",
    "\n",
    "        sources.append(node_index[lig])\n",
    "        targets.append(node_index[rec])\n",
    "        values.append(w)\n",
    "        colors.append(\"rgba(160,160,160,0.6)\")  # gray\n",
    "        hover.append(\n",
    "            f\"{r['lig']} → {r['rec']}\"\n",
    "            f\"<br>LR strength={float(r['lr_score']):.3f}\"\n",
    "            f\"<br>n_pairs={int(r['n_pairs'])}\"\n",
    "            f\"<br>OmniPath category: {r['omnipath_category']}\"\n",
    "        )\n",
    "\n",
    "    # Receptor → TF (signed Spearman, width = |r| → [0.5, 10])\n",
    "    for _, row in rec_tf_edges.iterrows():\n",
    "        rec_name = str(row[\"receptor\"])\n",
    "        tf_name  = str(row[\"TF\"])\n",
    "        s = f\"REC:{rec_name}\"\n",
    "        t = f\"TF:{tf_name}\"\n",
    "        if s not in node_index or t not in node_index:\n",
    "            continue\n",
    "\n",
    "        w = float(row[\"value\"])\n",
    "        if w <= 0:\n",
    "            continue\n",
    "\n",
    "        r_val = float(row[\"spearman_r\"])\n",
    "        sources.append(node_index[s])\n",
    "        targets.append(node_index[t])\n",
    "        values.append(w)\n",
    "        colors.append(\"rgba(220,20,60,0.35)\" if r_val >= 0 else \"rgba(30,144,255,0.35)\")\n",
    "        hover.append(f\"{rec_name} → {tf_name}<br>Spearman r={r_val:.3f}\")\n",
    "\n",
    "    # TF → Pathway (signed Spearman, width = |r| → [0.5, 10])\n",
    "    for _, row in tf_pw_edges.iterrows():\n",
    "        tf_name = str(row[\"TF\"])\n",
    "        pw_name = str(row[\"pathway\"])\n",
    "        s = f\"TF:{tf_name}\"\n",
    "        t = f\"PW:{pw_name}\"\n",
    "        if s not in node_index or t not in node_index:\n",
    "            continue\n",
    "\n",
    "        w = float(row[\"value\"])\n",
    "        if w <= 0:\n",
    "            continue\n",
    "\n",
    "        r_val = float(row[\"spearman_r\"])\n",
    "        sources.append(node_index[s])\n",
    "        targets.append(node_index[t])\n",
    "        values.append(w)\n",
    "        colors.append(\"rgba(220,20,60,0.35)\" if r_val >= 0 else \"rgba(30,144,255,0.35)\")\n",
    "        hover.append(f\"{tf_name} → {pw_name}<br>Spearman r={r_val:.3f}\")\n",
    "\n",
    "        pretty_labels = [n.split(\":\", 1)[1] for n in all_nodes]\n",
    "\n",
    "    fig = go.Figure(go.Sankey(\n",
    "        arrangement=\"snap\",\n",
    "        node=dict(\n",
    "            label=pretty_labels,\n",
    "            x=node_x,\n",
    "            y=node_y,\n",
    "            pad=12,\n",
    "            thickness=14,\n",
    "            color=node_colors,\n",
    "            line=dict(color=\"rgba(0,0,0,0.2)\", width=0.5),\n",
    "        ),\n",
    "        link=dict(\n",
    "            source=sources,\n",
    "            target=targets,\n",
    "            value=values,\n",
    "            color=colors,\n",
    "            customdata=hover,\n",
    "            hovertemplate=\"%{customdata}<extra></extra>\",\n",
    "        ),\n",
    "    ))\n",
    "\n",
    "    # set node label font globally for this Sankey trace\n",
    "    fig.update_traces(\n",
    "        textfont=dict(color=\"black\", size=14)\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"{condition_label}: {SOURCE_CT} → {TARGET_CT} (LIANA LR → TF(ULM) → PROGENy)\",\n",
    "        font=dict(size=14),\n",
    "        margin=dict(l=20, r=20, t=60, b=60),\n",
    "        height=400,\n",
    "        width=1000,\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "# Build Pattern and Random figures\n",
    "fig_pat = make_sankey_from_edges(\n",
    "    \"Pattern\",\n",
    "    lr_pat_edges, rec_tf_pat, tf_pw_pat,\n",
    "    lig_order=lig_global,\n",
    "    rec_order=rec_global,\n",
    "    tf_order=tf_global,\n",
    "    pw_order=pw_global,\n",
    ")\n",
    "\n",
    "fig_ran = make_sankey_from_edges(\n",
    "    \"Random\",\n",
    "    lr_ran_edges, rec_tf_ran, tf_pw_ran,\n",
    "    lig_order=lig_global,\n",
    "    rec_order=rec_global,\n",
    "    tf_order=tf_global,\n",
    "    pw_order=pw_global,\n",
    ")\n",
    "\n",
    "fig_pat.show()\n",
    "fig_ran.show()\n",
    "\n",
    "# Save HTML and edge tables\n",
    "pat_html = f\"{OUT_PREFIX}_Pattern.html\"\n",
    "ran_html = f\"{OUT_PREFIX}_Random.html\"\n",
    "fig_pat.write_html(pat_html)\n",
    "fig_ran.write_html(ran_html)\n",
    "print(\"Saved HTML:\", pat_html, ran_html)\n",
    "\n",
    "lr_pat_edges.to_csv(f\"{OUT_PREFIX}_Pattern_LR_edges.csv\", index=False)\n",
    "lr_ran_edges.to_csv(f\"{OUT_PREFIX}_Random_LR_edges.csv\", index=False)\n",
    "rec_tf_pat.to_csv(f\"{OUT_PREFIX}_Pattern_REC_to_TF_edges.csv\", index=False)\n",
    "rec_tf_ran.to_csv(f\"{OUT_PREFIX}_Random_REC_to_TF_edges.csv\", index=False)\n",
    "tf_pw_pat.to_csv(f\"{OUT_PREFIX}_Pattern_TF_to_PW_edges.csv\", index=False)\n",
    "tf_pw_ran.to_csv(f\"{OUT_PREFIX}_Random_TF_to_PW_edges.csv\", index=False)\n",
    "print(\"Saved edge tables (CSV).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfcb116-fed7-41bc-a1bd-822692638ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save sankey plots as .png Update height/width in previous block if needed\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.write_image(fig_pat, f\"{OUT_PREFIX}_Pattern.png\", format=\"png\", scale=4)\n",
    "pio.write_image(fig_ran, f\"{OUT_PREFIX}_Random.png\", format=\"png\", scale=4)\n",
    "\n",
    "print(\"Saved PNGs:\", f\"{OUT_PREFIX}_Pattern.png\", f\"{OUT_PREFIX}_Random.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad6c704-e70b-4aa2-aa3a-01fd070f3369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print package versions \n",
    "\n",
    "from importlib.metadata import version, PackageNotFoundError\n",
    "\n",
    "def get_version(pkg):\n",
    "    try:\n",
    "        return version(pkg)\n",
    "    except PackageNotFoundError:\n",
    "        return \"NOT INSTALLED\"\n",
    "\n",
    "print(\"Package / resource versions\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Core packages\n",
    "print(f\"LIANA:      {get_version('liana')}\")\n",
    "print(f\"DecoupleR:  {get_version('decoupler')}\")\n",
    "print(f\"CollecTRI:  {get_version('collectri')}\")\n",
    "print(f\"PROGENy:  {get_version('progeny')}\")\n",
    "\n",
    "# CollecTRI and PROGENy are bundled resources within decoupler\n",
    "# They do NOT have independent package versions\n",
    "print(\"*CollecTRI is provided via DecoupleR\")\n",
    "print(\"*PROGENy is provided via DecoupleR\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
